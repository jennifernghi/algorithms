{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\jenni\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (1.41.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jenni\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (1.22.2)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (3.18.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\jenni\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "70,000 grayscale images of 28 × 28 pixels each, with 10 classes\n",
    " the pixel intensities are represented as integers (from 0 to 255) \n",
    " rather than floats (from 0.0 to 255.0)\n",
    " \n",
    " \n",
    "\"\"\"\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dataset is already split into a training set and a test set, but there is no validation set, so we’ll create one now. Additionally, since we are going to train the neural network using Gradient Descent, we must scale the input features. For simplicity, we’ll scale the pixel intensities down to the 0–1 range by dividing them by 255.0 (this also converts them to floats):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Fashion MNIST, however, we need the list of class names to know what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first image in the training set represents a coat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification MLP with two hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The first line creates a Sequential model. \n",
    "This is the simplest kind of Keras model for neural networks \n",
    "that are just composed of a single stack of layers connected sequentially. \n",
    "This is called the Sequential API.\n",
    "\"\"\"\n",
    "model = keras.models.Sequential()\n",
    "\"\"\"\n",
    "Next, we build the first layer and add it to the model. \n",
    "It is a Flatten layer whose role is to convert each input image \n",
    "into a 1D array: if it receives input data X, \n",
    "it computes X.reshape(-1, 28*28). \n",
    "This layer does not have any parameters; \n",
    "it is just there to do some simple preprocessing. \n",
    "Since it is the first layer in the model, \n",
    "you should specify the input_shape, which doesn’t \n",
    "include the batch size, only the shape of the instances. \n",
    "Alternatively, you could add a keras.layers.InputLayer as the \n",
    "first layer, setting input_shape=[28,28].\n",
    "\"\"\"\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "\"\"\"\n",
    "Next we add a Dense hidden layer with 300 neurons. \n",
    "It will use the ReLU activation function. \n",
    "Each Dense layer manages its own weight matrix, \n",
    "containing all the connection weights between the neurons \n",
    "and their inputs. It also manages a vector of bias terms \n",
    "(one per neuron). When it receives some input data, \n",
    "it computes RELU\n",
    "\"\"\"\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "\"\"\"\n",
    "Then we add a second Dense hidden layer with 100 neurons, also using the ReLU activation function.\n",
    "\"\"\"\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "\"\"\"\n",
    "Finally, we add a Dense output layer with 10 neurons (one per class)\n",
    ", using the softmax activation function \n",
    "(because the classes are exclusive).\n",
    "\"\"\"\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model’s summary() method displays all the model’s layers, including each layer’s name\n",
    "\n",
    "Note that Dense layers often have a lot of parameters\n",
    "\n",
    "the first hidden layer has 784 × 300 connection weights, plus 300 bias terms, which adds up to 235,500 parameters\n",
    "\n",
    "This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily get a model’s list of layers, to fetch a layer by its index, or you can fetch it by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.flatten.Flatten at 0x18813cf4f40>,\n",
       " <keras.layers.core.dense.Dense at 0x18819974eb0>,\n",
       " <keras.layers.core.dense.Dense at 0x18819afb970>,\n",
       " <keras.layers.core.dense.Dense at 0x18819b50670>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the parameters of a layer can be accessed using its get_weights() and set_weights() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02298503 -0.00524017  0.04122557 ...  0.06297977  0.06131759\n",
      "   0.06239198]\n",
      " [-0.03740768  0.06873949  0.06478673 ... -0.05190772 -0.03348781\n",
      "  -0.05172724]\n",
      " [ 0.02867264  0.02156693  0.00892299 ...  0.01734243  0.01280714\n",
      "  -0.01115044]\n",
      " ...\n",
      " [-0.02574976  0.02313401 -0.05127238 ... -0.05165261  0.06724736\n",
      "  -0.04575659]\n",
      " [ 0.06140584  0.00193688  0.00528567 ...  0.04093091  0.04218265\n",
      "  -0.06759141]\n",
      " [ 0.02791654 -0.02682829 -0.0583418  ... -0.0654764  -0.00690526\n",
      "   0.02158599]]\n",
      "(784, 300)\n"
     ]
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "print(weights)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(biases)\n",
    "print(biases.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Dense layer initialized the connection weights randomly (which is needed to break symmetry\n",
    "\n",
    "and the biases were initialized to zeros, which is fine. If you ever want to use a different initialization method, you can set kernel_initializer (kernel is another name for the matrix of connection weights) or bias_initializer when creating the layer.\n",
    "\n",
    "The shape of the weight matrix depends on the number of inputs. \n",
    "\n",
    "This is why it is recommended to specify the input_shape when creating the first layer in a Sequential model. \n",
    "\n",
    "However, if you do not specify the input shape, it’s OK: Keras will simply wait until it knows the input shape before it actually builds the model. \n",
    "\n",
    "This will happen either when you feed it actual data (e.g., during training), or when you call its build() method. \n",
    "\n",
    "Until the model is really built, the layers will not have any weights, and you will not be able to do certain things (such as print the model summary or save the model). \n",
    "\n",
    "So, if you know the input shape when creating the model, it is best to specify it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a model is created, you must call its compile() method to \n",
    "specify the loss function and the optimizer to use. \n",
    "Optionally, you can specify a list of extra metrics to compute \n",
    "during training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\"\"\"\n",
    "First, we use the \"sparse_categorical_crossentropy\" loss because \n",
    "we have sparse labels (i.e., for each instance, there is just a \n",
    "target class index, from 0 to 9 in this case), \n",
    "and the classes are exclusive. \n",
    "If instead we had one target probability per class for \n",
    "each instance (such as one-hot vectors, \n",
    "e.g. [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), \n",
    "then we would need to use the \"categorical_crossentropy\" loss instead. \n",
    "If we were doing binary classification or multilabel binary \n",
    "classification, then we would use the \"sigmoid\" (i.e., logistic) activation \n",
    "function in the output layer instead of the \"softmax\" activation \n",
    "function, and we would use the \"binary_crossentropy\" loss.\n",
    "\"\"\"\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you want to convert sparse labels (i.e., class indices) to one-hot vector labels, use the keras.utils.to_categorical() function. To go the other way round, use the np.argmax() function with axis=1.\n",
    "\n",
    "Regarding the optimizer, \"sgd\" means that we will train the model using simple Stochastic Gradient Descent. In other words, Keras will perform the backpropagation algorithm described earlier (i.e., reverse-mode autodiff plus Gradient Descent). \n",
    "\n",
    "When using the SGD optimizer, it is important to tune the learning rate. So, you will generally want to use optimizer=keras.optimizers.SGD(lr=???) to set the learning rate, rather than optimizer=\"sgd\", which defaults to lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7141 - accuracy: 0.7703 - val_loss: 0.4967 - val_accuracy: 0.8340\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4852 - accuracy: 0.8300 - val_loss: 0.4442 - val_accuracy: 0.8496\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4431 - accuracy: 0.8447 - val_loss: 0.4157 - val_accuracy: 0.8600\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4161 - accuracy: 0.8547 - val_loss: 0.4078 - val_accuracy: 0.8566\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3959 - accuracy: 0.8610 - val_loss: 0.3794 - val_accuracy: 0.8708\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3807 - accuracy: 0.8656 - val_loss: 0.3789 - val_accuracy: 0.8680\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3684 - accuracy: 0.8695 - val_loss: 0.3690 - val_accuracy: 0.8688\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3560 - accuracy: 0.8733 - val_loss: 0.3559 - val_accuracy: 0.8766\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3457 - accuracy: 0.8775 - val_loss: 0.3519 - val_accuracy: 0.8726\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3368 - accuracy: 0.8795 - val_loss: 0.3478 - val_accuracy: 0.8822\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3284 - accuracy: 0.8822 - val_loss: 0.3527 - val_accuracy: 0.8738\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3205 - accuracy: 0.8851 - val_loss: 0.3319 - val_accuracy: 0.8852\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3126 - accuracy: 0.8882 - val_loss: 0.3573 - val_accuracy: 0.8744\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3066 - accuracy: 0.8899 - val_loss: 0.3361 - val_accuracy: 0.8804\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2995 - accuracy: 0.8923 - val_loss: 0.3233 - val_accuracy: 0.8822\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2926 - accuracy: 0.8949 - val_loss: 0.3366 - val_accuracy: 0.8818\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2871 - accuracy: 0.8960 - val_loss: 0.3321 - val_accuracy: 0.8818\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2817 - accuracy: 0.8984 - val_loss: 0.3278 - val_accuracy: 0.8822\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2764 - accuracy: 0.9002 - val_loss: 0.3189 - val_accuracy: 0.8888\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2710 - accuracy: 0.9024 - val_loss: 0.3201 - val_accuracy: 0.8864\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2671 - accuracy: 0.9035 - val_loss: 0.3179 - val_accuracy: 0.8870\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2613 - accuracy: 0.9062 - val_loss: 0.3068 - val_accuracy: 0.8918\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2577 - accuracy: 0.9057 - val_loss: 0.3034 - val_accuracy: 0.8914\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2519 - accuracy: 0.9102 - val_loss: 0.3020 - val_accuracy: 0.8884\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2482 - accuracy: 0.9099 - val_loss: 0.3109 - val_accuracy: 0.8886\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2441 - accuracy: 0.9122 - val_loss: 0.3016 - val_accuracy: 0.8906\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2397 - accuracy: 0.9131 - val_loss: 0.2947 - val_accuracy: 0.8920\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2353 - accuracy: 0.9149 - val_loss: 0.2924 - val_accuracy: 0.8926\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2308 - accuracy: 0.9173 - val_loss: 0.3057 - val_accuracy: 0.8922\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2285 - accuracy: 0.9163 - val_loss: 0.3100 - val_accuracy: 0.8880\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                     validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass it the input features (X_train) and the target classes (y_train), as well as the number of epochs to train (or else it would default to just 1, which would definitely not be enough to converge to a good solution). \n",
    "\n",
    "We also pass a validation set (this is optional). \n",
    "\n",
    "Keras will measure the loss and the extra metrics on this set at the end of each epoch, which is very useful to see how well the model really performs. \n",
    "\n",
    "If the performance on the training set is much better than on the validation set, your model is probably overfitting the training set \n",
    "\n",
    "And that’s it! The neural network is trained. \n",
    "\n",
    "At each epoch during training, Keras displays the number of instances processed so far (along with a progress bar), \n",
    "\n",
    "the mean training time per sample, and the loss and accuracy (or any other extra metrics you asked for) on both the training set and the validation set. \n",
    "\n",
    "You can see that the training loss went down, which is a good sign, and the validation accuracy reached 89.26% after 30 epochs. \n",
    "\n",
    "That’s not too far from the training accuracy, so there does not seem to be much overfitting going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of passing a validation set using the validation_data argument, you could set validation_split to the ratio of the training set that you want Keras to use for validation. For example, validation_split=0.1 tells Keras to use the last 10% of the data (before shuffling) for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
